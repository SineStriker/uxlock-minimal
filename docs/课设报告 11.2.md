# 课设报告

## 基础知识

### SMP(Symmetric Multi-Processor)

+ 对称多处理器结构，指服务器中多个CPU对称工作，每个CPU访问内存地址所需时间相同。其主要特征是共享，包含对CPU，内存，I/O等进行共享。

+ SMP能够保证内存一致性，但这些共享的资源很可能成为性能瓶颈，随着CPU数量的增加，每个CPU都要访问相同的内存资源，可能导致内存访问冲突，

+ 可能会导致CPU资源的浪费。常用的PC机就属于这种。

### NUMA(Non-Uniform Memory Access)

+ 非一致存储访问，将CPU分为CPU模块，每个CPU模块由多个CPU组成，并且具有独立的本地内存、I/O槽口等，模块之间可以通过互联模块相互访问，

+ 访问本地内存的速度将远远高于访问远地内存(系统内其它节点的内存)的速度，这也是非一致存储访问的由来。NUMA较好地解决SMP的扩展问题，

+ 当CPU数量增加时，因为访问远地内存的延时远远超过本地内存，系统性能无法线性增加。

###　CLH锁

+ （Craig, Landin, Hagersten三位作者的缩写）CLH队列锁是一种基于链表的可扩展、高性能、公平的自旋锁。申请线程仅仅在本地变量上自旋，它不断轮询前驱的状态，假设发现前驱释放了锁就结束自旋。CLH在SMP系统结构下非常有效。

+ 数据结构
    + QNode链表，tail域
    ```c
    struct QNode {
        bool locked;
        QNode *prev;
    };

    QNode *tail; // tail域
    ````

    + 原子函数
    ```c
    QNode *getAndSet(QNode **tail, QNode *cur) {
        QNode *tmp = *tail;
        *tail = cur;
        return tmp;
    }
    ````

    + 初始化
    ```c

    QNode *createNode() {
        QNode *node = (QNode *) malloc(sizeof(QNode));
        node->prev = nullptr;
        node->locked = false;
        return node;
    }

    tail = createNode(); // 哨兵节点
    ````

    + 线程拿锁
        + 调用`tail`域的`getAndSet`方法，使自己成为队列尾部，并获取其前驱节点的引用
        + 在前驱结点的`locked`字段上自旋
    ```c
    void lock(QNode *node) {
        node->locked = true; // 当前节点上锁
        node->prev = getAndSet(&tail, node); // 当前节点进入队尾
        while (node->prev->locked){
            // 自旋直到前驱节点放锁
        }
    }
    ````

    + 线程放锁
        + 将当前结点的`locked`域设置为`false`，同时回收前驱结点
    ```c
    void unlock(QNode *node) {
        node->locked = false; // 当前节点放锁

        QNode *tmp = node->prev;
        node->prev = node->prev->prev;
        free(tmp);
    }
    ```

+ 运转过程
    + 一开始，默认创建一个`tail`节点用来占位
    + 当一个线程要上锁时，它创建一个`node`节点并获得对其所有权，然后调用`lock`函数，此时`node`进入队尾并开始自旋
    + 当一个线程要放锁时，它调用`unlock`函数，然后放弃`node`的所有权，此时`node`仍在队列中但只有它的后继节点或`tail`拥有它的引用
    + 可以看出，一开始创建的用来占位的节点，会被第一个拿锁的线程释放；当所有节点都不需要锁了以后，`tail`指向的是最后一个拿锁的线程获取的节点，整个过程是自洽的。

+ 缺点
    + 在NUMA系统结构下，每个线程有自己的内存，如果前趋结点的内存位置比较远，自旋判断前趋结点的locked域，性能将大打折扣。

### MCS锁

+ 与CLH是在前趋结点的locked域上自旋等待不同的是，MCS是在自己的结点的locked域上自旋等待。因此，它解决了CLH在NUMA系统架构中获取locked域状态内存过远的问题。

+ 数据结构
    + QNode链表，tail域
    ```c
    struct QNode {
        bool locked;
        QNode *next;
    };

    QNode *tail; // tail域
    ````

    + 原子函数
    ```c
    QNode *getAndSet(QNode **tail, QNode *cur) {
        QNode *tmp = *tail;
        *tail = cur;
        return tmp;
    }
    ````

    + 初始化
    ```c

    QNode *createNode() {
        QNode *node = (QNode *) malloc(sizeof(QNode));
        node->next = nullptr;
        node->locked = false;
        return node;
    }

    tail = NULL; // 哨兵节点
    ````

    + 线程拿锁
        + 调用`tail`域的`getAndSet`方法，使自己成为队列尾部，如果原来`tail`域为空也就是说自己就是第一个节点，那么直接拿锁，否则自旋等待它的前驱节点将它的`locked`域设为`false`
    ```c
    void lock(QNode *node) {
        node->locked = true; // 当前节点上锁
        QNode *prev = getAndSet(&tail, node); // 当前节点进入队尾
        if (prev) {
            // 如果当前节点不是队头节点
            prev->next = node;
            while (node->locked){
                // 自旋直到它的前驱节点将它解锁
            }
        } else {
            // 直接成功拿锁
        }
    }
    ````

    + 线程放锁
        + 将后继结点的`locked`域设置为`false`，同时回收当前节点
    ```c
    void unlock(QNode *node) {
        if (node->next) {
            node->next->locked = false;
        }
        free(node);
    }
    ```

    + 运转过程
        + 比CLH锁好理解，也是内存平衡的。

## MutexEE

### 指标

+ 功耗（Power Consumption）

+ 利用率（Energy Efficiency）
    + 又称“单位能耗吞吐量”（TPP,throughput per power）

睡眠（挂起）有利于降低功耗，忙等（自旋）有利于提升利用率，因此需要采取折衷策略。

### 分析

+ 忙等
    + 本地自旋，速度较快，CPI较低，耗能较高；全局自旋耗能低；
    + 使用`nop`指令增加本地自旋CPI，但是起到了相反的效果；
    + 为了避免CPU乱序执行，考虑插入内存屏障；

+ 睡眠
    + 入睡与唤醒时延较高，在较短的临界区性能差；
    + 需要让一部分线程保持自旋，另一部分挂起；

### 优化后的锁

+ Mutex 的问题
    + 可能临界区的时间比`sleep`的系统调用时间短；
    + 在用户空间释放了锁之后，有可能唤醒的线程还没醒来，就有新的线程又获取了锁——公平性得不到保证。

+ 设计
    + 入睡前等待8000个时钟周期；
    + 使用mfence内存屏障串行，用来降低因为乱序执行带来的功耗；
    + 在放锁后会等待一段时间，检测锁是否被用户态另一个线程获取，如果是的话就不执行futex唤醒；

+ 模式
    + 自旋：拿锁时自旋8000周期，放锁时自旋384周期；
    + 睡眠：拿锁时自旋256周期，放锁时自旋128周期；

    + 这两种模式区别就是自旋和沉睡的比例不同，根据实际情况动态切换；

+ 特点
    + 在临界区的较短时，MUTEXEE在效率以及功耗上要大幅好于MUTEX，吞吐率较高；

## UXLock 概要

### 解决问题的关键

+ 传统锁策略的问题
    + pthread/futex：见上对Mutex的问题分析；
    + MCS锁：在QNode队列中，一个线程放锁后会去唤醒下一个线程，引入了唤醒开销，导致效率下降。

+ 目标
    + 鉴于忙等与睡眠在效率和功耗上各有所长，因此两种策略都要使用。
    + 主要矛盾在于如何设计一种规则，使得各线程能够合理在两种状态之间切换，在不导致饥饿的相对公平条件下，做到相对高效率与低功耗。

### 已有的思路 Proto

#### 策略

+ 维持两个队列，一个active（队列中所有线程都占有CPU资源），一个secondary（队列中所有线程都处于挂起状态）

+ 每个线程有一个属于自己的时间片

+ 当一个线程需要拿锁时，通过原子操作将代表自己的QNode插入active队尾

+ 只有active队首的线程持有锁

+ active后面所有的节点都处于忙等状态，等到自己的时间片用完后通过原子操作将自己移动到secondary队尾

+ 持有锁的线程完成临界区任务后，需要根据一定策略从secondary队列中唤醒某些线程使其重新加入active队列中

#### 问题

1. 如果secondary队列中的线程只能等待active队列中的线程来唤醒，而持有锁的线程在放锁时又一定会先选择active队列中其他某个处于忙等状态的线程将锁传递，这样就会导致secondary队列中的线程出现饥饿
    + 给挂起线程设置一个时间，使它能够自我唤醒，持有锁的线程放锁时优先将锁给睡眠后自我唤醒的线程（但无法保证这个线程与它不在同一个核心上）
    
2. 如果持有锁的线程需要在已有线程等待的情况下，浪费时间去唤醒被挂起的线程，会引起关键路径额外的开销

## 运行环境准备

### 主要工具

+ VSCode
+ Clangd

### 准备容器

+ 拉取仓库
````
git clone https://ipads.se.sjtu.edu.cn:1312/he11oliu/uxlock.git
cd uxlock
git checkout block-exp
````

+ 构建容器（在cmd中运行）
````
docker run -it --name uxlock -p 2023:22 -v %cd%:/home/uxlock ubuntu:20.04
````

+ 安装工具链
````
apt install gcc make clang llvm libpapi-dev libnuma-dev
````

### 准备IDE

+ 安装bear分析工具
````
apt install bear
````

+ 安装Clangd插件，关闭C/C++ Intellisense

+ 配置`settings.json`如下
````
{
    "C_Cpp.intelliSenseEngine": "Disabled",
    "clangd.arguments": [
        "--header-insertion=never",
        "--function-arg-placeholders=false",
        "--clang-tidy"
    ]
}
````

+ 生成`compile_commands.json`
````
cd liti
bear make
make clean
````

+ 补充gitignore
````
.cache
bin/
obj/
compile_commands.json
````


## 代码理解

### 模板

+ 批处理
    + 在外层`Makefile.config`中定义了所有的互斥量、条件变量、自旋锁与读写锁的策略，`Makefile`会遍历这些策略，将`interpose.c`编译成每一种策略的目标文件`.o`。在处理一次策略时，编译器预定义两个重要的宏，一个是`FCT_LINK_SUFFIX`（设为该策略的名字），另一个是该策略的头文件中指定的`LOCK_ALGORITHM`（设为有效）
    + 如果当前编译的是mcs策略，那么就有两个预定义的宏
        + `FCT_LINK_SUFFIX`=`mcs`
        + `MCS`

+ `interpose.h`
    + 声明了四种不同对象的指针，分别是互斥量、条件变量、自旋锁与读写锁，这些指针在的名字全部以`FCT_LINK_SUFFIX`为后缀

+ `interpose.c`
    + 在文件头，根据第二个预定义宏引入当前策略的头文件，比如mcs就引入`mcs.h`
    + `interpose.h`中定义的这组接口在中间正式定义，使用128字节对齐，这组接口在函数`interpose_init`中初始化（使用`dlsym`函数从动态库中寻找实现），由于该函数使用`__attribute__((constructor))`而会在`main`函数前执行
    + 将系统头文件`pthread.h`中的部分函数重写了实现，这样评测程序在调用`pthread`中的函数时就会重定位到`interpose`的实现中（默认还是链接了`pthread`系统库，在使用脚本评测时会进行替换）
    + 在`NO_INDIRECTION`开启时，使用那一组接口显式链接每个策略的动态库（运行时动态加载）；未开启时，操作系统隐式链接每个策略的动态库。项目中似乎都没有开启。

+ 自定义策略
    + 所有的策略头文件后面，都会使用`lock`为前缀的宏将策略的函数统一化为同一个名字，这样引入`interpose.c`时就可以用同一个名字，只要使用`LOCK_ALGORITHM`作为开关来选择各策略即可。