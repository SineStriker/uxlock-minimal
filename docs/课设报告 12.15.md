# 课设报告

## 基础知识

### SMP(Symmetric Multi-Processor)

+ 对称多处理器结构，指服务器中多个CPU对称工作，每个CPU访问内存地址所需时间相同。其主要特征是共享，包含对CPU，内存，I/O等进行共享。

+ SMP能够保证内存一致性，但这些共享的资源很可能成为性能瓶颈，随着CPU数量的增加，每个CPU都要访问相同的内存资源，可能导致内存访问冲突，

+ 可能会导致CPU资源的浪费。常用的PC机就属于这种。

### NUMA(Non-Uniform Memory Access)

+ 非一致存储访问，将CPU分为CPU模块，每个CPU模块由多个CPU组成，并且具有独立的本地内存、I/O槽口等，模块之间可以通过互联模块相互访问，

+ 访问本地内存的速度将远远高于访问远地内存(系统内其它节点的内存)的速度，这也是非一致存储访问的由来。NUMA较好地解决SMP的扩展问题，

+ 当CPU数量增加时，因为访问远地内存的延时远远超过本地内存，系统性能无法线性增加。

### TAS自旋锁
```c
do
{
    // 进入区
    while(Test_And_Set_Lock(&lock));
    // 临界区
    lock = False;
    // 剩余区
} while(True)
````

### CLH锁

+ （Craig, Landin, Hagersten三位作者的缩写）CLH队列锁是一种基于链表的可扩展、高性能、公平的自旋锁。申请线程仅仅在本地变量上自旋，它不断轮询前驱的状态，假设发现前驱释放了锁就结束自旋。CLH在SMP系统结构下非常有效。

+ 数据结构
    + QNode链表，tail域
    ```c
    struct QNode {
        bool locked;
        QNode *prev;
    };

    QNode *tail; // tail域
    ````

    + 原子函数
    ```c
    QNode *getAndSet(QNode **tail, QNode *cur) {
        QNode *tmp = *tail;
        *tail = cur;
        return tmp;
    }
    ````

    + 初始化
    ```c

    QNode *createNode() {
        QNode *node = (QNode *) malloc(sizeof(QNode));
        node->prev = nullptr;
        node->locked = false;
        return node;
    }

    tail = createNode(); // 哨兵节点
    ````

    + 线程拿锁
        + 调用`tail`域的`getAndSet`方法，使自己成为队列尾部，并获取其前驱节点的引用
        + 在前驱结点的`locked`字段上自旋
    ```c
    void lock(QNode *node) {
        node->locked = true; // 当前节点上锁
        node->prev = getAndSet(&tail, node); // 当前节点进入队尾
        while (node->prev->locked){
            // 自旋直到前驱节点放锁
        }
    }
    ````

    + 线程放锁
        + 将当前结点的`locked`域设置为`false`，同时回收前驱结点
    ```c
    void unlock(QNode *node) {
        node->locked = false; // 当前节点放锁

        QNode *tmp = node->prev;
        node->prev = node->prev->prev;
        free(tmp);
    }
    ```

+ 运转过程
    + 一开始，默认创建一个`tail`节点用来占位
    + 当一个线程要上锁时，它创建一个`node`节点并获得对其所有权，然后调用`lock`函数，此时`node`进入队尾并开始自旋
    + 当一个线程要放锁时，它调用`unlock`函数，然后放弃`node`的所有权，此时`node`仍在队列中但只有它的后继节点或`tail`拥有它的引用
    + 可以看出，一开始创建的用来占位的节点，会被第一个拿锁的线程释放；当所有节点都不需要锁了以后，`tail`指向的是最后一个拿锁的线程获取的节点，整个过程是自洽的。

+ 缺点
    + 在NUMA系统结构下，每个线程有自己的内存，如果前趋结点的内存位置比较远，自旋判断前趋结点的locked域，性能将大打折扣。

### MCS锁

+ 与CLH是在前趋结点的locked域上自旋等待不同的是，MCS是在自己的结点的locked域上自旋等待。因此，它解决了CLH在NUMA系统架构中获取locked域状态内存过远的问题。

+ 数据结构
    + QNode链表，tail域
    ```c
    struct QNode {
        bool locked;
        QNode *next;
    };

    QNode *tail; // tail域
    ````

    + 原子函数
    ```c
    QNode *getAndSet(QNode **tail, QNode *cur) {
        QNode *tmp = *tail;
        *tail = cur;
        return tmp;
    }
    ````

    + 初始化
    ```c

    QNode *createNode() {
        QNode *node = (QNode *) malloc(sizeof(QNode));
        node->next = nullptr;
        node->locked = false;
        return node;
    }

    tail = NULL; // 哨兵节点
    ````

    + 线程拿锁
        + 调用`tail`域的`getAndSet`方法，使自己成为队列尾部，如果原来`tail`域为空也就是说自己就是第一个节点，那么直接拿锁，否则自旋等待它的前驱节点将它的`locked`域设为`false`
    ```c
    void lock(QNode *node) {
        node->locked = true; // 当前节点上锁
        QNode *prev = getAndSet(&tail, node); // 当前节点进入队尾
        if (prev) {
            // 如果当前节点不是队头节点
            prev->next = node;
            while (node->locked){
                // 自旋直到它的前驱节点将它解锁
            }
        } else {
            // 直接成功拿锁
        }
    }
    ````

    + 线程放锁
        + 将后继结点的`locked`域设置为`false`，同时回收当前节点
    ```c
    void unlock(QNode *node) {
        if (node->next) {
            node->next->locked = false;
        }
        free(node);
    }
    ```

    + 运转过程
        + 比CLH锁好理解，也是内存平衡的。

## Malthusian

### 思路
+ 维护两个队列，Active（活跃队列）与Passive（阻塞队列）。
+ 让多余的线程睡眠并移出 Active 队列。
+ 在 Over Subscription 的情况下，重新排序一个类似 MCS 的队列，在保证锁被充分竞争的前提下，让部分线程睡眠，让部分锁能够重复快速拿锁，并保证长期公平性。

### 过程
1. 在拿锁的时候就是简单的 MCS 队列。
2. 在放锁的时候，如果当前线程和队尾之间还有线程，则放入一个后继线程进入 Passive 队列中。
3. 如果发现没有后继线程，从 Passive 队列中取出一个线程进入 Active。
4. 保证长期公平性，线程长时间进入 Passive 队列后会进入 Active 队列。

## MutexEE

### 指标

+ 功耗（Power Consumption）

+ 利用率（Energy Efficiency）
    + 又称“单位能耗吞吐量”（TPP,throughput per power）

睡眠（挂起）有利于降低功耗，忙等（自旋）有利于提升利用率，因此需要采取折衷策略。

### 分析

+ 忙等
    + 本地自旋，速度较快，CPI较低，耗能较高；全局自旋耗能低；
    + 使用`nop`指令增加本地自旋CPI，但是起到了相反的效果；
    + 为了避免CPU乱序执行，考虑插入内存屏障；

+ 睡眠
    + 入睡与唤醒时延较高，在较短的临界区性能差；
    + 需要让一部分线程保持自旋，另一部分挂起；

### 优化后的锁

+ Mutex 的问题
    + 可能临界区的时间比`sleep`的系统调用时间短；
    + 在用户空间释放了锁之后，有可能唤醒的线程还没醒来，就有新的线程又获取了锁——公平性得不到保证。

+ 设计
    + 入睡前等待8000个时钟周期；
    + 使用mfence内存屏障串行，用来降低因为乱序执行带来的功耗；
    + 在放锁后会等待一段时间，检测锁是否被用户态另一个线程获取，如果是的话就不执行futex唤醒；

+ 模式
    + 自旋：拿锁时自旋8000周期，放锁时自旋384周期；
    + 睡眠：拿锁时自旋256周期，放锁时自旋128周期；

    + 这两种模式区别就是自旋和沉睡的比例不同，根据实际情况动态切换；

+ 特点
    + 在临界区的较短时，MUTEXEE在效率以及功耗上要大幅好于MUTEX，吞吐率较高；

## UXLock 概要

### 解决问题的关键

+ 传统锁策略的问题
    + pthread/futex：见上对Mutex的问题分析；
    + MCS锁：在QNode队列中，一个线程放锁后会去唤醒下一个线程，引入了唤醒开销，导致效率下降。

+ 目标
    + 鉴于忙等与睡眠在效率和功耗上各有所长，因此两种策略都要使用。
    + 主要矛盾在于如何设计一种规则，使得各线程能够合理在两种状态之间切换，在不导致饥饿的相对公平条件下，做到相对高效率与低功耗。

### 已有的思路 Proto

#### 策略

+ 维持两个队列，一个active（队列中所有线程都占有CPU资源），一个secondary（队列中所有线程都处于挂起状态）

+ 每个线程有一个属于自己的时间片

+ 当一个线程需要拿锁时，通过原子操作将代表自己的QNode插入active队尾

+ 只有active队首的线程持有锁，并且队首的节点拥有secondary队列的指针

+ active队列后面所有的节点一开始都处于忙等状态，部分节点用完自己的时间片后就会进入阻塞态；

+ 放锁时，遍历active队列找到第一个处于活跃态的线程，将secondary队列指针与锁的所有权转移，那么中间跳过的所有阻塞态线程也都加到了secondary队列中

+ 持有锁的线程完成临界区任务后，需要根据一定策略从secondary队列中唤醒某些线程使其重新加入active队列中

#### 问题

1. 如果secondary队列中的线程只能等待active队列中的线程来唤醒，而持有锁的线程在放锁时又一定会先选择active队列中其他某个处于忙等状态的线程将锁传递，这样就会导致secondary队列中的线程出现饥饿
    + 给挂起线程设置一个时间，使它能够自我唤醒，持有锁的线程放锁时优先将锁给睡眠后自我唤醒的线程（但无法保证这个线程与它不在同一个核心上）
    
2. 如果持有锁的线程需要在已有线程等待的情况下，浪费时间去唤醒被挂起的线程，会引起关键路径额外的开销

## 运行环境准备

### 主要工具

+ VSCode
+ Clangd

### 准备容器

+ 拉取仓库
````
git clone https://ipads.se.sjtu.edu.cn:1312/he11oliu/uxlock.git
cd uxlock
git checkout block-exp
````

+ 构建容器（在cmd中运行）
````
docker run -it --name uxlock -p 2023:22 -v %cd%:/home/uxlock ubuntu:20.04
````

+ 安装工具链
````
apt install gcc make clang llvm libpapi-dev libnuma-dev
````

### 准备IDE

+ 安装bear分析工具
````
apt install bear
````

+ 安装Clangd插件，关闭C/C++ Intellisense

+ 配置`settings.json`如下
````
{
    "C_Cpp.intelliSenseEngine": "Disabled",
    "clangd.arguments": [
        "--header-insertion=never",
        "--function-arg-placeholders=false",
        "--clang-tidy"
    ]
}
````

+ 生成`compile_commands.json`
````
cd liti
bear make
make clean
````

+ 补充gitignore
````
.cache
bin/
obj/
compile_commands.json
````


## 代码理解

### 模板

+ 批处理
    + 在外层`Makefile.config`中定义了所有的互斥量、条件变量、自旋锁与读写锁的策略，`Makefile`会遍历这些策略，将`interpose.c`编译成每一种策略的目标文件`.o`。在处理一次策略时，编译器预定义两个重要的宏，一个是`FCT_LINK_SUFFIX`（设为该策略的名字），另一个是该策略的头文件中指定的`LOCK_ALGORITHM`（设为有效）
    + 如果当前编译的是mcs策略，那么就有两个预定义的宏
        + `FCT_LINK_SUFFIX`=`mcs`
        + `MCS`

+ `interpose.h`
    + 声明了四种不同对象的指针，分别是互斥量、条件变量、自旋锁与读写锁，这些指针在的名字全部以`FCT_LINK_SUFFIX`为后缀

+ `interpose.c`
    + 在文件头，根据第二个预定义宏引入当前策略的头文件，比如mcs就引入`mcs.h`
    + `interpose.h`中定义的这组接口在中间正式定义，使用128字节对齐，这组接口在函数`interpose_init`中初始化（使用`dlsym`函数从动态库中寻找实现），由于该函数使用`__attribute__((constructor))`而会在`main`函数前执行
    + 将系统头文件`pthread.h`中的部分函数重写了实现，这样评测程序在调用`pthread`中的函数时就会重定位到`interpose`的实现中（默认还是链接了`pthread`系统库，在使用脚本评测时会进行替换）
    + 在`NO_INDIRECTION`开启时，使用那一组接口显式链接每个策略的动态库（运行时动态加载）；未开启时，操作系统隐式链接每个策略的动态库。项目中似乎都没有开启。

+ 自定义策略
    + 所有的策略头文件后面，都会使用`lock`为前缀的宏将策略的函数统一化为同一个名字，这样引入`interpose.c`时就可以用同一个名字，只要使用`LOCK_ALGORITHM`作为开关来选择各策略即可。


### 评测

+ bench_block.c
    + 参数列表
        + `nb_thread`：线程数，通过`-t`指定，默认值20
        + `long_number_of_shared_variables`：长临界区长度（单位64位），通过`-g`指定，默认值5120
        + `short_number_of_shared_variables`：短临界区长度（单位64位），通过`-s`指定，默认值32
        + `short_thread_number`：短临界区线程数，通过`-S`指定
        + `delay`：拿锁时间间隔（单位`nop`指令数），通过`-d`指定，默认值100
        + `sleep_time`：主线程总时长（单位秒），通过`-T`指定，默认值3
        + `ux_num`：模拟用户线程数，通过`-u`指定
    + 未使用的参数
        + `possiblity`：通过`-p`指定
        + `target_latency`：通过`-l`指定，默认值100000

    + main
        + 初始化随机数种子，解析命令行参数
        + 初始化全局锁、长短临界数据区，初始化线程barrier
        + 主线程创建所有子线程后放弃CPU，等待所有子线程内部初始化完毕
        + 所有线程都到达barrier以后，主线程睡眠`delay`秒，子线程开始任务
        + 子线程每拿锁一次，使用`papi`库计算CPU周期数，并做记录，如此循环往复
        + 主线程将`global_stop`置为1
        + 最后主线程输出数据

    + thread_routine_transparent
        + 根据线程id计算（一般是取余）得出cpu核心id，然后设置cpu亲和度
        + 线程每次拿锁后记录时延，并存到利用`__thread`关键字得到的副本区域
        + 当主线程将`global_stop`置1后退出循环并返回

+ libxxx_xxx.sh
    + LD_PRELOAD
        + 通过修改`LD_PRELOAD`环境变量为自定义策略的动态库，使评测主程序优先使用`LD_PRELOAD`指定的库中实现的pthread相关函数而非系统线程库中的实现，来达到类似Windows中DLL注入类似的效果，便于使用同一份可执行文件测试不同的调度策略。

+ measure.sh
    + 输出各核心的拿锁次数、p99、p95、p999、p50、avg、tail延迟
    + 最后输出所有核心的p50、avg、p99、p999、tail延迟，和总体吞吐率
    （至于为什么输出顺序不一样，我也不知道）


## 基本策略测试

统计在线程数量分别为16、32、48、64，临界区长度分别为8、16、24...64的条件下，pthread、MCS、MutexEE三种策略的时延与吞吐率。

### 16线程下各指标随临界区长度（短）变化（Delay200）

<img src=./images/1.png width=50% /><img src=./images/2.png width=50% /><img src=./images/3.png width=50% /><img src=./images/4.png width=50% /><img src=./images/5.png width=50% /><img src=./images/6.png width=50% />

1. 显然，对任意一种策略，随临界区长度增加，平均时延呈上升趋势，吞吐量呈下降趋势。

2. 由于MutexEE的特性，存在大量入睡前等待的线程拿到了锁而导致少量已经入睡的线程一直拿不到锁，因此它的尾时延非常的高；并且，MutexEE在临界区长为48时，P999指标出现了意外的峰值，说明在这份MutexEE策略的实现所采用的入睡前与放锁后等待周期数，与CPU核心数，出现了类似共振的效应，使“尾时延”的比例扩展到了P999；但是在线程数升到64后，P999又降了下来，说明MutexEE策略下饥饿现象出现的概率随临界区长度的变化函数并非单调的，在临界区长为48时出现了极值

3. MCS的平均时延远大于Pthread与MutexEE，吞吐量最低，但是尾时延非常小，主要原因在于MCS是公平的，在保证不存在饥饿（尾时延不高）的情况下，平均时延和吞吐量都不优秀。

4. 在短临界区的情形下，MutexEE确实比PThread实现了更低的平均时延与更高的吞吐量，得益于它的设计特性；但是MutexEE的尾时延非常高，因为它更容易导致饥饿。

5. Pthread与MutexEE两种都是无法确保公平性的策略，MutexEE为了降低平均延迟对饥饿的容忍度比Pthread更高，也就是更为激进，因此它们各百分比的时延随临界区的变化趋势都不太稳定；而MCS因为保证了公平性，指标随临界区变化而上升或下降一直是严格的。


### 16线程下两种指标随临界区长度变化（Delay100）

+ 其他指标与Delay200基本一致

<img src=./images/7.png width=50% /><img src=./images/8.png width=50% />

6. 在Delay 100情形下，由于线程的竞争状态更为激烈，因此各策略的吞吐量相比Delay 200整体下降

7. 在Delay 100情形下，MutexEE的尾时延在临界区上升时变化得更剧烈，说明竞争状态越强MutexEE出现饥饿现象的概率增幅越大

### 8临界区长度下各指标随线程数变化（Delay200）

<img src=./images/9.png width=50% /><img src=./images/10.png width=50% /><img src=./images/11.png width=50% /><img src=./images/12.png width=50% /><img src=./images/13.png width=50% /><img src=./images/14.png width=50% />

8. 临界区长度一定时，各指标随线程数量的变化，基本都趋于线性变化

9. 在没有出现OverSubscribe现象时，线程数量较少，Pthread策略表限较好，但是线程数量增加后，Pthread的时延（除了尾时延）比MCS和MutexEE上升得更快，尤其是P99与P999，由于MutexEE擅长临界区较短的情况，Pthread的时延（除了尾时延）都比MutexEE高；

### 16线程下各指标随临界区长度（长）变化（Delay200）
<img src=./images/15.png width=50% /><img src=./images/16.png width=50% /><img src=./images/17.png width=50% /><img src=./images/18.png width=50% /><img src=./images/19.png width=50% /><img src=./images/20.png width=50% />

10. 在临界区长度较长时，Pthread与MCS变得逐渐趋近——由于本身时间就长，线程数不多，Pthread导致的饥饿现象随临界区长度变长而变得不明显；

11. 在临界区长度较长时，MutexEE虽然能够保证平均时延比另外两种更低，但是其表现显然比如临界区短时优异了

12. MutexEE的P99与P999在临界区长度变长时有个突然上升的过程，说明瓶颈已到，其尾时延比临界区较短时更为糟糕

## ACE Lock 思路（随便取的名字）

### 思路

1. MCS锁是一种十分精巧的策略，但它与任何一种阻塞策略搭配，最后的结果都不那么令人满意；如果使用忙等，那么整个等待队列中所有线程都在忙等，但是后面的大量线程明显是没有必要处于自旋状态的，它们的忙等不仅浪费能量，还会抢占持有锁线程的资源；如果使用阻塞，那么前面的线程明明很快能获取锁却还要经历唤醒，产生没有必要的时延；本策略在综合分析MCS的特性之后提出，与MCS的思想非常类似；

2. 策略原则为，每个核心上只存在一个处于活跃态的线程，其他线程全部阻塞。存在一个MCS主队列，主队列上排列的是所有活跃线程；对于一个核心，核心上的所有线程排成一个MCS队列，队头是当前核心上的活跃线程，其他为阻塞线程；

3. 当有新的线程要获取锁，它首先通过原子操作使自己进入自己核心的队列，如果它是核心中的第一个线程，那么再通过原子操作使自己进入主队列，否则进入阻塞态；进入主队列后，如果它是主队列第一个线程，那么直接获取锁，否则进入自旋态；

4. 线程被解除自旋态，到获取锁之前，它需要将自己核心队列的下一个线程唤醒，那么下一个线程就会自己进入主队列；

5. 线程放锁时，将主队列的下一个线程解旋，那么下一个线程又唤醒自己核心的下一个线程...

6. 本策略本质上是一个双层的MCS队列，主队列是MCS队列，每个核心自己的队列也是MCS队列。

### 伪代码实现

+ 数据结构
    ```c
    struct QNode {
        int cpu_id;
        QNode *block_next;       // 同CPU阻塞队列后继节点
        QNode *active_next;      // 活跃队列后继节点
    };
    
    QNode *activeTail;           // 活跃队列尾节点
    QNode *coreTails[n];         // 每个CPU阻塞队列尾节点
    ````

+ 上锁
    ```c
    void lock(QNode *node) {
        QNode *org = getAndSet(&coreTails[node->cpu_id], node);
        if (org) {
            // 当前CPU存在活跃节点
            org->block_next = node;
            // 转换为阻塞态...
            sleep();
        }

        // 进入active队尾
        QNode *prev = getAndSet(&activeTail, node); // 进入阻塞队尾
        if (prev) {
            // 如果当前节点不是队头节点
            prev->activeNext = node;
            // 转换为自旋态
            spin();
        }
        // 唤醒当前CPU处于当前节点后面的节点
        awake(node->block_next);
    }
    ````

+ 解锁
    ```c
    void unlock(QNode *node) {
        // 解除活跃队列后继节点的自旋态，传递锁
        despin(node->active_next);
    }
    ````